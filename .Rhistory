library(validate)
validate_original_schema <- function(df) {
rules <- validator(
sepal_length > 0,
sepal_width > 0,
petal_length > 0,
petal_width > 0,
class %in% c("setosa", "versicolor", "virginica")
)
summary(confront(df, rules))
}
library(dplyr)
inspect_missing <- function(df) {
sapply(df, function(x) sum(is.na(x)))
}
remove_duplicates <- function(df) {
df %>% distinct()
}
cap_outliers <- function(x) {
Q1 <- quantile(x, 0.25)
Q3 <- quantile(x, 0.75)
IQR <- Q3 - Q1
lower <- Q1 - 1.5 * IQR
upper <- Q3 + 1.5 * IQR
pmin(pmax(x, lower), upper)
}
feature_engineering <- function(df) {
df %>%
mutate(
petal_to_sepal_length = petal_length / sepal_length,
petal_to_sepal_width  = petal_width / sepal_width,
sepal_ratio = sepal_length / sepal_width
) %>%
select(
petal_to_sepal_length,
petal_to_sepal_width,
petal_length,
petal_width,
sepal_ratio,
class
)
}
library(caret)
library(dplyr)
source("src/validate.R")
source("src/preprocess.R")
data(iris)
df <- iris %>%
rename(
sepal_length = Sepal.Length,
sepal_width  = Sepal.Width,
petal_length = Petal.Length,
petal_width  = Petal.Width,
class = Species
) %>%
mutate(class = tolower(as.character(class)))
validate_original_schema(df)
df <- remove_duplicates(df)
df_features <- feature_engineering(df)
df_features$class <- factor(
df_features$class,
levels = c("setosa", "versicolor", "virginica")
)
write.csv(df_features, "data/iris-processed.csv", row.names = FALSE)
set.seed(66)
index <- createDataPartition(df_features$class, p = 0.8, list = FALSE)
train <- df_features[index, ]
test  <- df_features[-index, ]
scaler <- preProcess(train[, -6], method = c("center", "scale"))
X_train <- predict(scaler, train[, -6])
X_test  <- predict(scaler, test[, -6])
y_train <- train$class
y_test  <- test$class
ctrl <- trainControl(method = "cv", number = 5)
rf_model <- train(
x = X_train,
y = y_train,
method = "rf",
trControl = ctrl
)
saveRDS(rf_model, "artifacts/model.rds")
saveRDS(scaler, "artifacts/scaler.rds")
predict_new <- function(new_data) {
model <- readRDS("artifacts/model.rds")
scaler <- readRDS("artifacts/scaler.rds")
new_scaled <- predict(scaler, new_data)
predict(model, new_scaled)
}
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(corrplot)
library(naniar)
data <- read.csv("../../data_artifacts/raw/data.csv")
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(naniar)
library(randomForest)
data(iris)
df <- iris %>%
rename(
sepal_length = Sepal.Length,
sepal_width  = Sepal.Width,
petal_length = Petal.Length,
petal_width  = Petal.Width,
class = Species
) %>%
mutate(class = tolower(as.character(class)))
na_summary <- sapply(df, function(x) sum(is.na(x)))
print("Missing values per column:")
print(na_summary)
# Optional visualization
vis_miss(df)
duplicates <- duplicated(df)
print(paste("Number of duplicate rows:", sum(duplicates)))
df <- df[!duplicates, ]
numeric_cols <- names(df)[sapply(df, is.numeric)]
for(col in numeric_cols){
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
lower <- Q1 - 1.5 * IQR
upper <- Q3 + 1.5 * IQR
df[[col]][df[[col]] < lower] <- lower
df[[col]][df[[col]] > upper] <- upper
}
summary_stats <- summary(df[numeric_cols])
print("Summary statistics of numeric columns:")
print(summary_stats)
cor_matrix <- cor(df[numeric_cols])
print("Correlation matrix:")
print(cor_matrix)
high_cor <- which(abs(cor_matrix) > 0.8 & abs(cor_matrix) < 1, arr.ind = TRUE)
if(nrow(high_cor) > 0){
print("Highly correlated feature pairs:")
for(i in 1:nrow(high_cor)){
r <- high_cor[i,]
cat(rownames(cor_matrix)[r[1]], "-", colnames(cor_matrix)[r[2]],
":", round(cor_matrix[r[1], r[2]],2), "\n")
}
}
# Pairplot
pairs(df[numeric_cols])
# Histograms
for(col in numeric_cols){
print(
ggplot(df, aes_string(col)) +
geom_histogram(bins=30, fill="steelblue", color="black") +
theme_minimal()
)
}
# Class distribution
ggplot(df, aes(x=class)) +
geom_bar(fill="steelblue") +
theme_minimal()
df_features <- df
df_features$class <- factor(df_features$class, levels = c("setosa", "versicolor", "virginica"))
write.csv(df_features, "data/iris-processed.csv", row.names = FALSE)
print("Processed dataset saved to data/iris-processed.csv")
set.seed(66)
index <- createDataPartition(df_features$class, p = 0.8, list = FALSE)
train <- df_features[index, ]
test  <- df_features[-index, ]
scaler <- preProcess(train[, numeric_cols], method = c("center", "scale"))
X_train <- predict(scaler, train[, numeric_cols])
X_test  <- predict(scaler, test[, numeric_cols])
y_train <- train$class
y_test  <- test$class
ctrl <- trainControl(method = "cv", number = 5)
rf_model <- train(
x = X_train,
y = y_train,
method = "rf",
trControl = ctrl
)
pred <- predict(rf_model, newdata = X_test)
conf <- confusionMatrix(pred, y_test)
print("Confusion Matrix and Metrics:")
print(conf)
print("Cross-validation results:")
print(rf_model)
saveRDS(rf_model, "artifacts/model.rds")
saveRDS(scaler, "artifacts/scaler.rds")
print("Model and scaler saved to artifacts/")
library(validate)
validate_original_schema <- function(df) {
rules <- validator(
sepal_length > 0,
sepal_width > 0,
petal_length > 0,
petal_width > 0,
class %in% c("setosa", "versicolor", "virginica")
)
result <- confront(df, rules)
summary_result <- summary(result)
print(summary_result)
# Optional: print rows failing each rule
for(rule_name in names(result)){
failing_rows <- which(!as.logical(result[[rule_name]]))
if(length(failing_rows) > 0){
cat("\nRows failing rule", rule_name, ":", paste(failing_rows, collapse = ", "), "\n")
}
}
return(invisible(summary_result))
}
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(naniar)
library(randomForest)
data(iris)
df <- iris %>%
rename(
sepal_length = Sepal.Length,
sepal_width  = Sepal.Width,
petal_length = Petal.Length,
petal_width  = Petal.Width,
class = Species
) %>%
mutate(class = tolower(as.character(class)))
na_summary <- sapply(df, function(x) sum(is.na(x)))
print("Missing values per column:")
print(na_summary)
# Optional visualization
vis_miss(df)
duplicates <- duplicated(df)
print(paste("Number of duplicate rows:", sum(duplicates)))
df <- df[!duplicates, ]
numeric_cols <- names(df)[sapply(df, is.numeric)]
for(col in numeric_cols){
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
lower <- Q1 - 1.5 * IQR
upper <- Q3 + 1.5 * IQR
df[[col]][df[[col]] < lower] <- lower
df[[col]][df[[col]] > upper] <- upper
}
summary_stats <- summary(df[numeric_cols])
print("Summary statistics of numeric columns:")
print(summary_stats)
cor_matrix <- cor(df[numeric_cols])
print("Correlation matrix:")
print(cor_matrix)
high_cor <- which(abs(cor_matrix) > 0.8 & abs(cor_matrix) < 1, arr.ind = TRUE)
if(nrow(high_cor) > 0){
print("Highly correlated feature pairs:")
for(i in 1:nrow(high_cor)){
r <- high_cor[i,]
cat(rownames(cor_matrix)[r[1]], "-", colnames(cor_matrix)[r[2]],
":", round(cor_matrix[r[1], r[2]],2), "\n")
}
}
# Pairplot
pairs(df[numeric_cols])
# Histograms
for(col in numeric_cols){
print(
ggplot(df, aes_string(col)) +
geom_histogram(bins=30, fill="steelblue", color="black") +
theme_minimal()
)
}
# Class distribution
ggplot(df, aes(x=class)) +
geom_bar(fill="steelblue") +
theme_minimal()
df_features <- df
df_features$class <- factor(df_features$class, levels = c("setosa", "versicolor", "virginica"))
write.csv(df_features, "data/iris-processed.csv", row.names = FALSE)
print("Processed dataset saved to data/iris-processed.csv")
set.seed(66)
index <- createDataPartition(df_features$class, p = 0.8, list = FALSE)
train <- df_features[index, ]
test  <- df_features[-index, ]
scaler <- preProcess(train[, numeric_cols], method = c("center", "scale"))
X_train <- predict(scaler, train[, numeric_cols])
X_test  <- predict(scaler, test[, numeric_cols])
y_train <- train$class
y_test  <- test$class
ctrl <- trainControl(method = "cv", number = 5)
rf_model <- train(
x = X_train,
y = y_train,
method = "rf",
trControl = ctrl
)
pred <- predict(rf_model, newdata = X_test)
conf <- confusionMatrix(pred, y_test)
print("Confusion Matrix and Metrics:")
print(conf)
print("Cross-validation results:")
print(rf_model)
saveRDS(rf_model, "artifacts/model.rds")
saveRDS(scaler, "artifacts/scaler.rds")
print("Model and scaler saved to artifacts/")
- R >= 4.0
Install missing packages:
```r
- R >= 4.0
# Make sure your working directory is the project root
setwd("path/to/mlops-r")  # replace with your project path
getwd()
source("src/validate.R")
validate_original_schema(raw_data)
source("src/train.R")
source("src/predict.R")
predictions <- predict_new(new_data = read.csv("data/iris-processed.csv"))
head(predictions)
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(naniar)
library(randomForest)
library(validate)
# Load raw dataset
raw_data <- iris %>%
rename(
sepal_length = Sepal.Length,
sepal_width  = Sepal.Width,
petal_length = Petal.Length,
petal_width  = Petal.Width,
class = Species
) %>%
mutate(class = tolower(as.character(class)))
cat("Libraries loaded and raw_data initialized.\n")
validate_original_schema <- function(df) {
rules <- validator(
sepal_length > 0,
sepal_width > 0,
petal_length > 0,
petal_width > 0,
class %in% c("setosa", "versicolor", "virginica")
)
result <- confront(df, rules)
summary_result <- summary(result)
print(summary_result)
# Print failing rows per rule
for(rule_name in names(result)){
failing_rows <- which(!as.logical(result[[rule_name]]))
if(length(failing_rows) > 0){
cat("\nRows failing rule", rule_name, ":", paste(failing_rows, collapse = ", "), "\n")
}
}
return(invisible(summary_result))
}
source("src/init.R")
source("src/validate.R")
# 1. Validate raw data
validate_original_schema(raw_data)
# 2. Copy raw_data for processing
df <- raw_data
# 3. Check for missing values
na_summary <- sapply(df, function(x) sum(is.na(x)))
print("Missing values per column:")
print(na_summary)
vis_miss(df)
# 4. Remove duplicates
duplicates <- duplicated(df)
cat("Number of duplicate rows:", sum(duplicates), "\n")
df <- df[!duplicates, ]
# 5. Outlier detection and capping
numeric_cols <- names(df)[sapply(df, is.numeric)]
for(col in numeric_cols){
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
lower <- Q1 - 1.5 * IQR
upper <- Q3 + 1.5 * IQR
df[[col]][df[[col]] < lower] <- lower
df[[col]][df[[col]] > upper] <- upper
}
# 6. Summary statistics
summary_stats <- summary(df[numeric_cols])
print("Summary statistics of numeric columns:")
print(summary_stats)
# 7. Correlation analysis
cor_matrix <- cor(df[numeric_cols])
print("Correlation matrix:")
print(cor_matrix)
high_cor <- which(abs(cor_matrix) > 0.8 & abs(cor_matrix) < 1, arr.ind = TRUE)
if(nrow(high_cor) > 0){
print("Highly correlated feature pairs:")
for(i in 1:nrow(high_cor)){
r <- high_cor[i,]
cat(rownames(cor_matrix)[r[1]], "-", colnames(cor_matrix)[r[2]], ":", round(cor_matrix[r[1], r[2]],2), "\n")
}
}
# 8. EDA visualizations
pairs(df[numeric_cols])
for(col in numeric_cols){
print(
ggplot(df, aes_string(col)) +
geom_histogram(bins=30, fill="steelblue", color="black") +
theme_minimal()
)
}
ggplot(df, aes(x=class)) + geom_bar(fill="steelblue") + theme_minimal()
# 9. Feature engineering (placeholder if needed)
df_features <- df
df_features$class <- factor(df_features$class, levels = c("setosa", "versicolor", "virginica"))
# 10. Save processed dataset
write.csv(df_features, "data/iris-processed.csv", row.names = FALSE)
cat("Processed dataset saved to data/iris-processed.csv\n")
# 11. Train/test split
set.seed(66)
index <- createDataPartition(df_features$class, p = 0.8, list = FALSE)
train <- df_features[index, ]
test  <- df_features[-index, ]
# 12. Scaling numeric features
scaler <- preProcess(train[, numeric_cols], method = c("center", "scale"))
X_train <- predict(scaler, train[, numeric_cols])
X_test  <- predict(scaler, test[, numeric_cols])
y_train <- train$class
y_test  <- test$class
# 13. Train Random Forest with 5-fold CV
ctrl <- trainControl(method = "cv", number = 5)
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = ctrl)
# 14. Evaluation
pred <- predict(rf_model, newdata = X_test)
conf <- confusionMatrix(pred, y_test)
print("Confusion Matrix and Metrics:")
print(conf)
print("Cross-validation results:")
print(rf_model)
# 15. Save artifacts
saveRDS(rf_model, "artifacts/model.rds")
saveRDS(scaler, "artifacts/scaler.rds")
cat("Model and scaler saved to artifacts/\n")
predict_new <- function(new_data) {
model <- readRDS("artifacts/model.rds")
scaler <- readRDS("artifacts/scaler.rds")
numeric_cols <- c("sepal_length", "sepal_width", "petal_length", "petal_width")
if(!all(numeric_cols %in% colnames(new_data))){
stop("Missing required numeric columns in new_data")
}
new_scaled <- predict(scaler, new_data[, numeric_cols])
preds <- predict(model, new_scaled)
result <- cbind(new_data, predicted_class = preds)
return(result)
}
src/train.R
cd
src/train.R
list.files("artifacts")
library(plumber)
src/train.R
source("src/train.R")
install.packages(c(
"caret",
"tidyverse",
"mlflow",
"nnet",
"randomForest",
"rpart",
"e1071",
"plumber"
))
source("src/train.R")
install.packages("ggplot2")
install.packages("caret")
source("src/train.R")
library(mlflow)
source("src/train.R")
source("src/train.R")
install.packages("MLmetrics")
source("src/train.R")
source("src/train.R")
list.files("artifacts")
library(plumber)
pr <- plumb("api/plumber.R")
library(plumber)
pr <- plumb("api/plumber.R")
getwd()
list.files("artifacts")
library(plumber)
pr <- plumb("api/plumber.R")
rstudioapi::getActiveDocumentContext()$path
library(plumber)
pr <- plumb("api/plumber.R")
install.packages("here")
library(plumber)
pr <- plumb("api/plumber.R")
library(plumber)
pr <- plumb("api/plumber.R")
pr$run(port = 7860)
library(plumber)
pr <- plumb("api/plumber.R")
pr$run(port = 7860)
pr$stop()
pr <- plumb("api/plumber.R")
pr$run(port = 7860)
source("src/train.R")
pr$stop()
library(plumber)
pr <- plumb("api/plumber.R")
pr$run(port = 7860)
source("src/train.R")
list.files("artifacts")
# model.rds, scaler.rds, feature_names.rds
library(plumber)
pr <- plumb("api/plumber.R")
pr$run(port = 7860)
source("src/train.R")
library(plumber)
pr <- plumb("api/plumber.R")
pr$run(port = 7860)
